//
// @file TXParallelProcessor.hpp
// @brief é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†æ¡†æ¶ - ä¸“ä¸ºXLSXæ–‡ä»¶æ“ä½œä¼˜åŒ–
//

#pragma once

#include "TXTypes.hpp"
#include "TXResult.hpp"
#include "TXCoordinate.hpp"
#include "TXMemoryPool.hpp"
#include <vector>
#include <thread>
#include <future>
#include <functional>
#include <memory>
#include <atomic>
#include <queue>
#include <condition_variable>
#include <mutex>
#include <chrono>
#include <unordered_map>

namespace TinaXlsx {

/**
 * @brief å¹¶è¡Œä»»åŠ¡æ¥å£
 */
class TXParallelTask {
public:
    virtual ~TXParallelTask() = default;
    virtual void execute() = 0;
    virtual std::string getTaskName() const = 0;
};

/**
 * @brief ğŸš€ æ— é”é«˜æ€§èƒ½çº¿ç¨‹æ± 
 *
 * ç‰¹ç‚¹ï¼š
 * - å·¥ä½œçªƒå–ç®—æ³•å‡å°‘é”ç«äº‰
 * - çº¿ç¨‹æœ¬åœ°é˜Ÿåˆ—æå‡æ€§èƒ½
 * - å†…å­˜æ± é›†æˆå‡å°‘åˆ†é…å¼€é”€
 * - ä»»åŠ¡ä¼˜å…ˆçº§æ”¯æŒ
 */
class TXLockFreeThreadPool {
public:
    enum class TaskPriority {
        Low = 0,
        Normal = 1,
        High = 2,
        Critical = 3
    };

    struct PoolConfig {
        size_t numThreads = std::thread::hardware_concurrency();
        size_t queueCapacity = 1024;
        bool enableWorkStealing = true;
        bool enableMemoryPool = true;
        size_t memoryPoolBlockSize = 256;
    };

    explicit TXLockFreeThreadPool(const PoolConfig& config = PoolConfig{});
    ~TXLockFreeThreadPool();

    /**
     * @brief æäº¤ä»»åŠ¡ï¼ˆæ”¯æŒä¼˜å…ˆçº§ï¼‰
     */
    template<typename F, typename... Args>
    auto submit(F&& f, Args&&... args, TaskPriority priority = TaskPriority::Normal)
        -> std::future<typename std::result_of<F(Args...)>::type> {
        using ReturnType = typename std::result_of<F(Args...)>::type;

        auto task = std::make_shared<std::packaged_task<ReturnType()>>(
            std::bind(std::forward<F>(f), std::forward<Args>(args)...)
        );

        std::future<ReturnType> result = task->get_future();

        if (!submitTaskInternal([task](){ (*task)(); }, priority)) {
            throw std::runtime_error("Failed to submit task to thread pool");
        }

        return result;
    }

    /**
     * @brief æ‰¹é‡æäº¤ä»»åŠ¡
     */
    template<typename Iterator>
    std::vector<std::future<void>> submitBatch(Iterator begin, Iterator end,
                                              TaskPriority priority = TaskPriority::Normal) {
        std::vector<std::future<void>> futures;
        futures.reserve(std::distance(begin, end));

        for (auto it = begin; it != end; ++it) {
            futures.push_back(submit(*it, priority));
        }

        return futures;
    }

    /**
     * @brief ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
     */
    void waitForAll();

    /**
     * @brief è·å–æ€§èƒ½ç»Ÿè®¡
     */
    struct PoolStats {
        size_t totalTasksProcessed = 0;
        size_t tasksInQueue = 0;
        size_t activeThreads = 0;
        double averageTaskTime = 0.0;
        size_t workStealingCount = 0;
        std::chrono::microseconds totalProcessingTime{0};
    };

    PoolStats getStats() const;

    /**
     * @brief åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°é‡
     */
    void resizeThreadPool(size_t newSize);

private:
    struct Task {
        std::function<void()> function;
        TaskPriority priority;
        std::chrono::steady_clock::time_point submitTime;

        Task(std::function<void()> f, TaskPriority p)
            : function(std::move(f)), priority(p), submitTime(std::chrono::steady_clock::now()) {}
    };

    struct ThreadLocalQueue {
        std::queue<Task> tasks;
        std::mutex mutex;
        std::condition_variable condition;
        std::atomic<size_t> taskCount{0};
    };

    PoolConfig config_;
    std::vector<std::thread> workers_;
    std::vector<std::unique_ptr<ThreadLocalQueue>> localQueues_;
    std::atomic<bool> stop_{false};
    std::atomic<size_t> nextQueueIndex_{0};

    // æ€§èƒ½ç»Ÿè®¡
    mutable std::atomic<size_t> totalTasksProcessed_{0};
    mutable std::atomic<size_t> workStealingCount_{0};
    mutable std::atomic<std::chrono::microseconds::rep> totalProcessingTime_{0};

    // å†…å­˜æ± 
    std::unique_ptr<TXMemoryPool> memoryPool_;

    bool submitTaskInternal(std::function<void()> task, TaskPriority priority);
    void workerThread(size_t threadId);
    bool tryStealTask(size_t thiefId, Task& stolenTask);
    ThreadLocalQueue* selectQueue();
};

/**
 * @brief ğŸš€ æ™ºèƒ½å¹¶è¡Œå•å…ƒæ ¼å¤„ç†å™¨
 *
 * ç‰¹ç‚¹ï¼š
 * - è‡ªé€‚åº”æ‰¹é‡å¤§å°
 * - å†…å­˜æ± é›†æˆ
 * - è´Ÿè½½å‡è¡¡
 * - ç¼“å­˜å‹å¥½çš„æ•°æ®åˆ†å¸ƒ
 */
class TXSmartParallelCellProcessor {
public:
    struct ProcessorConfig {
        size_t numThreads = std::thread::hardware_concurrency();
        size_t minBatchSize = 100;
        size_t maxBatchSize = 10000;
        bool enableAdaptiveBatching = true;
        bool enableMemoryPool = true;
        bool enableCacheOptimization = true;
    };

    explicit TXSmartParallelCellProcessor(const ProcessorConfig& config = ProcessorConfig{});
    ~TXSmartParallelCellProcessor();

    /**
     * @brief æ™ºèƒ½å¹¶è¡Œè®¾ç½®å•å…ƒæ ¼å€¼
     */
    template<typename CellManager>
    TXResult<size_t> parallelSetCellValues(
        CellManager& manager,
        const std::vector<std::pair<TXCoordinate, cell_value_t>>& values
    ) {
        if (values.empty()) {
            return Ok(static_cast<size_t>(0));
        }

        // ğŸš€ è‡ªé€‚åº”æ‰¹é‡å¤§å°è®¡ç®—
        size_t optimalBatchSize = calculateOptimalBatchSize(values.size());

        // ğŸš€ ç¼“å­˜å‹å¥½çš„æ•°æ®é‡æ’åº
        auto sortedValues = config_.enableCacheOptimization ?
            sortForCacheEfficiency(values) : values;

        // ğŸš€ è´Ÿè½½å‡è¡¡çš„ä»»åŠ¡åˆ†é…
        auto batches = createBalancedBatches(sortedValues, optimalBatchSize);

        // å¹¶è¡Œå¤„ç†
        std::vector<std::future<size_t>> futures;
        futures.reserve(batches.size());

        for (const auto& batch : batches) {
            auto future = threadPool_->submit([&manager, batch]() -> size_t {
                // ä½¿ç”¨æ‰¹é‡æ“ä½œï¼Œä¸€æ¬¡æ€§è·å–é”
                return manager.setCellValues(batch);
            }, TXLockFreeThreadPool::TaskPriority::High);

            futures.push_back(std::move(future));
        }

        // æ”¶é›†ç»“æœ
        size_t totalCount = 0;
        for (auto& future : futures) {
            try {
                totalCount += future.get();
            } catch (const std::exception& e) {
                return Err<size_t>(TXErrorCode::OperationFailed,
                                 "Smart parallel processing failed: " + std::string(e.what()));
            }
        }

        // æ›´æ–°è‡ªé€‚åº”å‚æ•°
        updateAdaptiveParameters(values.size(), totalCount);

        return Ok(totalCount);
    }
    
    /**
     * @brief å¹¶è¡Œè®¾ç½®èŒƒå›´å€¼
     */
    template<typename CellManager>
    TXResult<size_t> parallelSetRangeValues(
        CellManager& manager,
        row_t startRow, column_t startCol,
        const std::vector<std::vector<cell_value_t>>& values
    ) {
        if (values.empty()) {
            return Ok(static_cast<size_t>(0));
        }
        
        // æŒ‰è¡Œå¹¶è¡Œå¤„ç†
        std::vector<std::future<size_t>> futures;
        futures.reserve(values.size());
        
        for (size_t rowIdx = 0; rowIdx < values.size(); ++rowIdx) {
            auto future = threadPool_->submit([&manager, startRow, startCol, &values, rowIdx]() -> size_t {
                row_t currentRow = row_t(startRow.index() + rowIdx);
                return manager.setRowValues(currentRow, startCol, values[rowIdx]);
            });

            futures.push_back(std::move(future));
        }
        
        // æ”¶é›†ç»“æœ
        size_t totalCount = 0;
        for (auto& future : futures) {
            try {
                totalCount += future.get();
            } catch (const std::exception& e) {
                return Err<size_t>(TXErrorCode::OperationFailed,
                                 "Parallel range processing failed: " + std::string(e.what()));
            }
        }
        
        return Ok(totalCount);
    }

private:
    ProcessorConfig config_;
    std::unique_ptr<TXLockFreeThreadPool> threadPool_;

    // ç§æœ‰è¾…åŠ©æ–¹æ³•
    size_t calculateOptimalBatchSize(size_t totalItems) const;
    std::vector<std::pair<TXCoordinate, cell_value_t>> sortForCacheEfficiency(
        const std::vector<std::pair<TXCoordinate, cell_value_t>>& values) const;
    std::vector<std::vector<std::pair<TXCoordinate, cell_value_t>>> createBalancedBatches(
        const std::vector<std::pair<TXCoordinate, cell_value_t>>& values, size_t batchSize) const;
    void updateAdaptiveParameters(size_t totalItems, size_t processedItems);
};

/**
 * @brief å¹¶è¡ŒXMLç”Ÿæˆå™¨
 * 
 * æ”¯æŒå¹¶è¡Œç”Ÿæˆå¤šä¸ªXMLæ–‡ä»¶
 */
class TXParallelXmlGenerator {
public:
    explicit TXParallelXmlGenerator(size_t numThreads = std::thread::hardware_concurrency());
    
    /**
     * @brief XMLç”Ÿæˆä»»åŠ¡
     */
    struct XmlGenerationTask {
        std::string partName;
        std::function<TXResult<std::string>()> generator;
        std::promise<TXResult<std::string>> promise;
        
        XmlGenerationTask(const std::string& name, 
                         std::function<TXResult<std::string>()> gen)
            : partName(name), generator(std::move(gen)) {}
    };
    
    /**
     * @brief æäº¤XMLç”Ÿæˆä»»åŠ¡
     */
    std::future<TXResult<std::string>> submitXmlTask(
        const std::string& partName,
        std::function<TXResult<std::string>()> generator
    );
    
    /**
     * @brief å¹¶è¡Œç”Ÿæˆå¤šä¸ªXMLæ–‡ä»¶
     */
    TXResult<std::vector<std::pair<std::string, std::string>>> generateXmlFiles(
        const std::vector<std::pair<std::string, std::function<TXResult<std::string>()>>>& tasks
    );

private:
    std::unique_ptr<TXLockFreeThreadPool> threadPool_;
};

/**
 * @brief å¹¶è¡Œå‹ç¼©å¤„ç†å™¨
 * 
 * æ”¯æŒå¹¶è¡Œå‹ç¼©å¤šä¸ªæ–‡ä»¶åˆ°ZIP
 */
class TXParallelZipProcessor {
public:
    explicit TXParallelZipProcessor(size_t numThreads = std::thread::hardware_concurrency());
    
    /**
     * @brief å‹ç¼©ä»»åŠ¡
     */
    struct CompressionTask {
        std::string filename;
        std::vector<uint8_t> data;
        int compressionLevel;
        
        CompressionTask(const std::string& name, 
                       std::vector<uint8_t> content,
                       int level = 6)
            : filename(name), data(std::move(content)), compressionLevel(level) {}
    };
    
    /**
     * @brief å¹¶è¡Œå‹ç¼©æ–‡ä»¶
     */
    TXResult<std::vector<std::pair<std::string, std::vector<uint8_t>>>> compressFiles(
        const std::vector<CompressionTask>& tasks
    );

private:
    std::unique_ptr<TXLockFreeThreadPool> threadPool_;

    std::vector<uint8_t> compressData(const std::vector<uint8_t>& data, int level);
};

/**
 * @brief å¹¶è¡Œå¤„ç†ç®¡ç†å™¨
 * 
 * ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å¹¶è¡Œå¤„ç†ç»„ä»¶
 */
class TXParallelProcessingManager {
public:
    TXParallelProcessingManager();
    ~TXParallelProcessingManager() = default;
    
    /**
     * @brief è·å–å•å…ƒæ ¼å¤„ç†å™¨
     */
    TXSmartParallelCellProcessor& getCellProcessor() { return cellProcessor_; }
    
    /**
     * @brief è·å–XMLç”Ÿæˆå™¨
     */
    TXParallelXmlGenerator& getXmlGenerator() { return xmlGenerator_; }
    
    /**
     * @brief è·å–ZIPå¤„ç†å™¨
     */
    TXParallelZipProcessor& getZipProcessor() { return zipProcessor_; }
    
    /**
     * @brief è®¾ç½®çº¿ç¨‹æ•°é‡
     */
    void setThreadCount(size_t numThreads);
    
    /**
     * @brief è·å–æ€§èƒ½ç»Ÿè®¡
     */
    struct PerformanceStats {
        size_t totalTasksProcessed = 0;
        std::chrono::microseconds totalProcessingTime{0};
        double averageTaskTime = 0.0;
        size_t activeThreads = 0;
        size_t queuedTasks = 0;
    };
    
    PerformanceStats getPerformanceStats() const;
    
    /**
     * @brief å¯ç”¨/ç¦ç”¨å¹¶è¡Œå¤„ç†
     */
    void setParallelProcessingEnabled(bool enabled) { parallelEnabled_ = enabled; }
    
    /**
     * @brief æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
     */
    bool isParallelProcessingEnabled() const { return parallelEnabled_; }

private:
    TXSmartParallelCellProcessor cellProcessor_;
    TXParallelXmlGenerator xmlGenerator_;
    TXParallelZipProcessor zipProcessor_;

    std::atomic<bool> parallelEnabled_;
    mutable std::atomic<size_t> totalTasksProcessed_;
    mutable std::chrono::microseconds totalProcessingTime_{0};
};

} // namespace TinaXlsx
