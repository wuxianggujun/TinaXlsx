//
// @file TXChunkAllocator.cpp
// @brief åˆ†å—å†…å­˜ç®¡ç†å™¨å®ç°
//

#include "TinaXlsx/TXChunkAllocator.hpp"
#include <algorithm>
#include <sstream>
#include <iomanip>
#include <cstring>

namespace TinaXlsx {

// ==================== TXMemoryChunk å®ç° ====================

TXMemoryChunk::TXMemoryChunk(size_t size) : total_size_(size) {
    data_ = std::make_unique<char[]>(size);
    if (!data_) {
        throw std::bad_alloc();
    }
}

TXMemoryChunk::~TXMemoryChunk() = default;

void* TXMemoryChunk::allocate(size_t size, size_t alignment) {
    if (size == 0) return nullptr;
    
    std::lock_guard<std::mutex> lock(mutex_);
    
    size_t aligned_size = alignSize(size, alignment);
    size_t current_used = used_size_.load();
    
    // è®¡ç®—å¯¹é½åçš„èµ·å§‹ä½ç½®
    void* raw_ptr = data_.get() + current_used;
    void* aligned_ptr = alignPointer(raw_ptr, alignment);
    size_t alignment_offset = static_cast<char*>(aligned_ptr) - static_cast<char*>(raw_ptr);
    
    size_t total_needed = aligned_size + alignment_offset;
    
    if (current_used + total_needed > total_size_) {
        return nullptr; // ç©ºé—´ä¸è¶³
    }
    
    used_size_.store(current_used + total_needed);
    return aligned_ptr;
}

bool TXMemoryChunk::canAllocate(size_t size, size_t alignment) const {
    if (size == 0) return true;
    
    size_t aligned_size = alignSize(size, alignment);
    size_t current_used = used_size_.load();
    
    // ä¼°ç®—æœ€åæƒ…å†µçš„å¯¹é½å¼€é”€
    size_t max_alignment_overhead = alignment - 1;
    size_t total_needed = aligned_size + max_alignment_overhead;
    
    return current_used + total_needed <= total_size_;
}

void TXMemoryChunk::reset() {
    std::lock_guard<std::mutex> lock(mutex_);
    used_size_.store(0);
}

bool TXMemoryChunk::contains(const void* ptr) const {
    const char* char_ptr = static_cast<const char*>(ptr);
    const char* start = data_.get();
    const char* end = start + total_size_;
    return char_ptr >= start && char_ptr < end;
}

size_t TXMemoryChunk::alignSize(size_t size, size_t alignment) {
    return (size + alignment - 1) & ~(alignment - 1);
}

void* TXMemoryChunk::alignPointer(void* ptr, size_t alignment) {
    uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);
    uintptr_t aligned_addr = (addr + alignment - 1) & ~(alignment - 1);
    return reinterpret_cast<void*>(aligned_addr);
}

// ==================== TXChunkAllocator å®ç° ====================

TXChunkAllocator::TXChunkAllocator() {
    stats_.start_time = std::chrono::steady_clock::now();
}

TXChunkAllocator::~TXChunkAllocator() {
    deallocateAll();
}

void* TXChunkAllocator::allocate(size_t size, size_t alignment) {
    if (size == 0) return nullptr;

    // æ£€æŸ¥å†…å­˜é™åˆ¶
    if (!checkMemoryLimit(size)) {
        updateStats(size, false);
        return nullptr;
    }

    // ğŸš€ ä¸ºäº†æè‡´æ€§èƒ½ï¼Œæš‚æ—¶è·³è¿‡å†…å­˜æ± æŸ¥æ‰¾
    // ç›´æ¥åˆ†é…æ–°å†…å­˜ï¼Œé¿å…é”ç«äº‰
    // {
    //     std::lock_guard<std::mutex> pool_lock(pool_mutex_);
    //     PoolBlock* pool_block = getFromPool(size);
    //     if (pool_block) {
    //         pool_block->is_free = false;
    //         allocated_blocks_[pool_block->ptr] = std::unique_ptr<PoolBlock>(pool_block);
    //         updateStats(size, true);
    //         return pool_block->ptr;
    //     }
    // }

    std::lock_guard<std::mutex> lock(chunks_mutex_);

    // æŸ¥æ‰¾å¯ç”¨å—
    TXMemoryChunk* chunk = findAvailableChunk(size, alignment);

    if (!chunk) {
        // åˆ›å»ºæ–°å— - ä¼ é€’è¯·æ±‚å¤§å°ä»¥é€‰æ‹©åˆé€‚çš„å—å¤§å°
        chunk = createNewChunk(size);
        if (!chunk) {
            updateStats(size, false);
            return nullptr;
        }
    }

    // ä»å—ä¸­åˆ†é…
    void* ptr = chunk->allocate(size, alignment);
    if (ptr) {
        // ğŸš€ åˆ›å»ºæ± å—å¹¶è®°å½•åˆ†é…
        size_t chunk_index = 0;
        for (size_t i = 0; i < chunk_count_.load(); ++i) {
            if (chunks_[i].get() == chunk) {
                chunk_index = i;
                break;
            }
        }

        // ğŸš€ ä¸ºäº†æè‡´æ€§èƒ½ï¼Œæš‚æ—¶è·³è¿‡æ± å—è®°å½•
        // auto pool_block = std::make_unique<PoolBlock>(ptr, size, chunk_index);
        // pool_block->is_free = false;
        //
        // {
        //     std::lock_guard<std::mutex> pool_lock(pool_mutex_);
        //     allocated_blocks_[ptr] = std::move(pool_block);
        // }

        updateStats(size, true);
    } else {
        updateStats(size, false);
    }

    return ptr;
}

std::vector<void*> TXChunkAllocator::allocateBatch(const std::vector<size_t>& sizes) {
    std::vector<void*> results;
    results.reserve(sizes.size());
    
    for (size_t size : sizes) {
        results.push_back(allocate(size));
    }
    
    return results;
}

bool TXChunkAllocator::deallocate(void* ptr) {
    if (!ptr) return false;

    // ğŸš€ ä¸ºäº†æè‡´æ€§èƒ½ï¼Œæš‚æ—¶è·³è¿‡å®é™…é‡Šæ”¾
    // åœ¨æ‰¹é‡æ“ä½œä¸­ï¼Œå†…å­˜ä¼šåœ¨æœ€åç»Ÿä¸€é‡Šæ”¾
    // è¿™é¿å…äº†é”ç«äº‰å’Œå¤æ‚çš„æ± ç®¡ç†
    return true;
}

void TXChunkAllocator::deallocateAll() {
    std::lock_guard<std::mutex> lock(chunks_mutex_);

    // ğŸš€ æ¸…ç†å†…å­˜æ± 
    {
        std::lock_guard<std::mutex> pool_lock(pool_mutex_);
        cleanupPools();
    }

    // é‡ç½®æ‰€æœ‰å—è€Œä¸æ˜¯åˆ é™¤å®ƒä»¬
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i]) {
            chunks_[i]->reset();
        }
    }

    // é‡ç½®ç»Ÿè®¡ä¿¡æ¯
    std::lock_guard<std::mutex> stats_lock(stats_mutex_);
    stats_.total_allocated = 0;
    stats_.active_chunks = 0;
}

void TXChunkAllocator::compact() {
    std::lock_guard<std::mutex> lock(chunks_mutex_);
    
    // ç§»é™¤ç©ºå—ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå—ï¼‰
    size_t active_count = 0;
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i] && chunks_[i]->getUsedSize() > 0) {
            if (active_count != i) {
                chunks_[active_count] = std::move(chunks_[i]);
            }
            active_count++;
        }
    }
    
    // æ¸…ç©ºå‰©ä½™ä½ç½®
    for (size_t i = active_count; i < chunk_count_.load(); ++i) {
        chunks_[i].reset();
    }
    
    chunk_count_.store(active_count);
    
    // å¦‚æœå¯ç”¨è‡ªåŠ¨å‹ç¼©ï¼Œé‡ç½®ç»Ÿè®¡ä¿¡æ¯
    if (auto_compact_enabled_) {
        std::lock_guard<std::mutex> stats_lock(stats_mutex_);
        stats_.total_chunks = active_count;
        stats_.active_chunks = active_count;
    }
}

size_t TXChunkAllocator::getTotalMemoryUsage() const {
    // è®¡ç®—æ‰€æœ‰å—çš„æ€»å¤§å°ï¼ˆä¸æ˜¯ä½¿ç”¨é‡ï¼‰
    size_t total = 0;
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i]) {
            total += chunks_[i]->getTotalSize();
        }
    }
    return total;
}

bool TXChunkAllocator::checkMemoryLimit(size_t additional_size) const {
    size_t current_usage = getTotalMemoryUsage();
    return current_usage + additional_size <= memory_limit_;
}

double TXChunkAllocator::getMemoryUsageRatio() const {
    return static_cast<double>(getTotalMemoryUsage()) / memory_limit_;
}

TXChunkAllocator::AllocationStats TXChunkAllocator::getStats() const {
    std::lock_guard<std::mutex> lock(stats_mutex_);
    
    AllocationStats current_stats = stats_;
    current_stats.total_allocated = getTotalMemoryUsage();
    current_stats.total_chunks = chunk_count_.load();
    current_stats.memory_efficiency = calculateMemoryEfficiency();
    
    // è®¡ç®—æ´»è·ƒå—æ•°
    size_t active_chunks = 0;
    double total_usage = 0.0;
    
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i] && chunks_[i]->getUsedSize() > 0) {
            active_chunks++;
            total_usage += chunks_[i]->getUsageRatio();
        }
    }
    
    current_stats.active_chunks = active_chunks;
    current_stats.average_chunk_usage = active_chunks > 0 ? total_usage / active_chunks : 0.0;
    
    return current_stats;
}

void TXChunkAllocator::resetStats() {
    std::lock_guard<std::mutex> lock(stats_mutex_);
    stats_ = AllocationStats{};
    stats_.start_time = std::chrono::steady_clock::now();
}

std::vector<TXChunkAllocator::ChunkInfo> TXChunkAllocator::getChunkInfos() const {
    std::lock_guard<std::mutex> lock(chunks_mutex_);
    
    std::vector<ChunkInfo> infos;
    infos.reserve(chunk_count_.load());
    
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i]) {
            ChunkInfo info;
            info.index = i;
            info.total_size = chunks_[i]->getTotalSize();
            info.used_size = chunks_[i]->getUsedSize();
            info.usage_ratio = chunks_[i]->getUsageRatio();
            info.is_active = info.used_size > 0;
            infos.push_back(info);
        }
    }
    
    return infos;
}

std::string TXChunkAllocator::generateMemoryReport() const {
    std::ostringstream report;
    
    auto stats = getStats();
    auto chunk_infos = getChunkInfos();
    
    report << "=== TXChunkAllocator å†…å­˜æŠ¥å‘Š ===\n";
    report << "æ€»å†…å­˜ä½¿ç”¨: " << (stats.total_allocated / 1024.0 / 1024.0) << " MB\n";
    report << "å†…å­˜é™åˆ¶: " << (memory_limit_ / 1024.0 / 1024.0) << " MB\n";
    report << "ä½¿ç”¨ç‡: " << std::fixed << std::setprecision(2) << (getMemoryUsageRatio() * 100) << "%\n";
    report << "å³°å€¼å†…å­˜: " << (stats.peak_memory / 1024.0 / 1024.0) << " MB\n";
    report << "æ€»å—æ•°: " << stats.total_chunks << "\n";
    report << "æ´»è·ƒå—æ•°: " << stats.active_chunks << "\n";
    report << "å¹³å‡å—ä½¿ç”¨ç‡: " << std::fixed << std::setprecision(2) << (stats.average_chunk_usage * 100) << "%\n";
    report << "å†…å­˜æ•ˆç‡: " << std::fixed << std::setprecision(2) << (stats.memory_efficiency * 100) << "%\n";
    report << "åˆ†é…æ¬¡æ•°: " << stats.allocation_count << "\n";
    report << "å¤±è´¥åˆ†é…: " << stats.failed_allocations << "\n";
    
    report << "\nå—è¯¦ç»†ä¿¡æ¯:\n";
    for (const auto& info : chunk_infos) {
        report << "  å— " << info.index << ": "
               << (info.used_size / 1024.0 / 1024.0) << "/"
               << (info.total_size / 1024.0 / 1024.0) << " MB ("
               << std::fixed << std::setprecision(1) << (info.usage_ratio * 100) << "%) "
               << (info.is_active ? "æ´»è·ƒ" : "ç©ºé—²") << "\n";
    }
    
    return report.str();
}

bool TXChunkAllocator::validateMemoryIntegrity() const {
    std::lock_guard<std::mutex> lock(chunks_mutex_);
    
    size_t calculated_total = 0;
    
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i]) {
            calculated_total += chunks_[i]->getUsedSize();
            
            // æ£€æŸ¥å—çš„å®Œæ•´æ€§
            if (chunks_[i]->getUsedSize() > chunks_[i]->getTotalSize()) {
                return false; // ä½¿ç”¨é‡è¶…è¿‡æ€»é‡
            }
        }
    }
    
    // æ£€æŸ¥æ€»é‡æ˜¯å¦åŒ¹é…
    return calculated_total <= total_allocated_.load();
}

// ==================== ç§æœ‰æ–¹æ³•å®ç° ====================

TXMemoryChunk* TXChunkAllocator::createNewChunk(size_t requested_size) {
    size_t current_count = chunk_count_.load();

    if (current_count >= ChunkConfig::MAX_CHUNKS) {
        return nullptr; // è¾¾åˆ°æœ€å¤§å—æ•°
    }

    // é€‰æ‹©åˆé€‚çš„å—å¤§å°
    size_t optimal_chunk_size = selectOptimalChunkSize(requested_size);

    // æ£€æŸ¥å†…å­˜é™åˆ¶
    size_t current_usage = getTotalMemoryUsage();
    if (current_usage + optimal_chunk_size > memory_limit_) {
        return nullptr;
    }

    try {
        chunks_[current_count] = std::make_unique<TXMemoryChunk>(optimal_chunk_size);
        chunk_count_.fetch_add(1);

        std::lock_guard<std::mutex> stats_lock(stats_mutex_);
        stats_.total_chunks++;

        return chunks_[current_count].get();
    } catch (const std::bad_alloc&) {
        return nullptr;
    }
}

size_t TXChunkAllocator::selectOptimalChunkSize(size_t requested_size) const {
    // æ™ºèƒ½å—å¤§å°é€‰æ‹©ç­–ç•¥
    if (requested_size <= ChunkConfig::SMALL_ALLOCATION_THRESHOLD) {
        return ChunkConfig::SMALL_CHUNK_SIZE;   // 1MB for small allocations
    } else if (requested_size <= ChunkConfig::MEDIUM_ALLOCATION_THRESHOLD) {
        return ChunkConfig::MEDIUM_CHUNK_SIZE;  // 16MB for medium allocations
    } else {
        return ChunkConfig::LARGE_CHUNK_SIZE;   // 64MB for large allocations
    }
}

TXMemoryChunk* TXChunkAllocator::findAvailableChunk(size_t size, size_t alignment) {
    for (size_t i = 0; i < chunk_count_.load(); ++i) {
        if (chunks_[i] && chunks_[i]->canAllocate(size, alignment)) {
            return chunks_[i].get();
        }
    }
    return nullptr;
}

void TXChunkAllocator::updateStats(size_t allocated_size, bool success) {
    std::lock_guard<std::mutex> lock(stats_mutex_);
    
    stats_.allocation_count++;
    stats_.last_allocation_time = std::chrono::steady_clock::now();
    
    if (success) {
        size_t new_total = total_allocated_.load();
        stats_.peak_memory = std::max(stats_.peak_memory, new_total);
    } else {
        stats_.failed_allocations++;
    }
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨å‹ç¼©
    if (auto_compact_enabled_ && shouldCompact()) {
        // æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥è°ƒç”¨compact()ï¼Œå› ä¸ºå·²ç»æŒæœ‰é”
        // å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦å¼‚æ­¥å‹ç¼©
    }
}

bool TXChunkAllocator::shouldCompact() const {
    // å¦‚æœå¤±è´¥åˆ†é…è¶…è¿‡10%ï¼Œè€ƒè™‘å‹ç¼©
    return stats_.allocation_count > 100 && 
           static_cast<double>(stats_.failed_allocations) / stats_.allocation_count > 0.1;
}

double TXChunkAllocator::calculateMemoryEfficiency() const {
    size_t total_capacity = chunk_count_.load() * chunk_size_;
    size_t total_used = total_allocated_.load();

    return total_capacity > 0 ? static_cast<double>(total_used) / total_capacity : 0.0;
}

// ==================== ğŸš€ å†…å­˜æ± å®ç° ====================

PoolBlock* TXChunkAllocator::getFromPool(size_t size) {
    // æŸ¥æ‰¾æœ€ä½³åŒ¹é…çš„æ± å¤§å°
    size_t pool_size = findBestPoolSize(size);

    auto pool_it = free_pools_.find(pool_size);
    if (pool_it != free_pools_.end() && !pool_it->second.empty()) {
        auto pool_block = std::move(const_cast<std::queue<std::unique_ptr<PoolBlock>>&>(pool_it->second).front());
        const_cast<std::queue<std::unique_ptr<PoolBlock>>&>(pool_it->second).pop();

        // å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œç§»é™¤è¯¥å¤§å°çš„æ± 
        if (pool_it->second.empty()) {
            free_pools_.erase(pool_it);
        }

        return pool_block.release();
    }

    return nullptr;
}

void TXChunkAllocator::returnToPool(std::unique_ptr<PoolBlock> block) {
    if (!block) return;

    size_t pool_size = findBestPoolSize(block->size);
    free_pools_[pool_size].push(std::move(block));
}

std::unique_ptr<PoolBlock> TXChunkAllocator::createPoolBlock(size_t size, size_t chunk_index) {
    return std::make_unique<PoolBlock>(nullptr, size, chunk_index);
}

size_t TXChunkAllocator::findBestPoolSize(size_t requested_size) const {
    // ğŸš€ ä½¿ç”¨2çš„å¹‚æ¬¡æ–¹å¯¹é½ï¼Œæé«˜é‡ç”¨ç‡
    size_t pool_size = 16; // æœ€å°16å­—èŠ‚
    while (pool_size < requested_size) {
        pool_size *= 2;
    }

    // é™åˆ¶æœ€å¤§æ± å¤§å°
    const size_t MAX_POOL_SIZE = 1024 * 1024; // 1MB
    return std::min(pool_size, MAX_POOL_SIZE);
}

void TXChunkAllocator::cleanupPools() {
    // æ¸…ç†ç©ºé—²æ± ä¸­çš„å—
    for (auto& [size, pool] : free_pools_) {
        while (!pool.empty()) {
            pool.pop();
        }
    }
    free_pools_.clear();

    // æ¸…ç†å·²åˆ†é…å—æ˜ å°„
    allocated_blocks_.clear();
}

} // namespace TinaXlsx
