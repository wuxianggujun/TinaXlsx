//
// @file TXAsyncProcessingFramework.cpp
// @brief å¼‚æ­¥å¤„ç†æ¡†æ¶å®ç°
//

#include "TinaXlsx/TXAsyncProcessingFramework.hpp"
#include <algorithm>
#include <sstream>
#include <iomanip>

namespace TinaXlsx {

// ==================== TXAsyncProcessingFramework å®ç° ====================

TXAsyncProcessingFramework::TXAsyncProcessingFramework(TXUnifiedMemoryManager& memory_manager,
                                                       const FrameworkConfig& config)
    : memory_manager_(memory_manager), config_(config) {
    
    // åˆå§‹åŒ–æ— é”é˜Ÿåˆ—
    normal_queue_ = std::make_unique<TXLockFreeQueue<std::unique_ptr<TXAsyncTask>>>(config_.task_queue_capacity);
    high_priority_queue_ = std::make_unique<TXLockFreeQueue<std::unique_ptr<TXAsyncTask>>>(config_.high_priority_queue_capacity);
}

TXAsyncProcessingFramework::~TXAsyncProcessingFramework() {
    if (state_.load() == FrameworkState::RUNNING) {
        stop();
    }
}

TXResult<void> TXAsyncProcessingFramework::start() {
    FrameworkState expected = FrameworkState::STOPPED;
    if (!state_.compare_exchange_strong(expected, FrameworkState::STARTING)) {
        return Err(TXErrorCode::Unknown, "Framework is not in STOPPED state");
    }

    try {
        should_stop_.store(false);

        // ç¡®å®šå·¥ä½œçº¿ç¨‹æ•°
        size_t thread_count = config_.worker_thread_count;
        if (thread_count == 0) {
            thread_count = getOptimalThreadCount();
        }

        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        worker_threads_.reserve(thread_count);
        for (size_t i = 0; i < thread_count; ++i) {
            worker_threads_.emplace_back(&TXAsyncProcessingFramework::workerThreadFunction, this, i);
        }

        state_.store(FrameworkState::RUNNING);

        return Ok();

    } catch (const std::exception& e) {
        state_.store(FrameworkState::STOPPED);
        return Err(TXErrorCode::Unknown, "Failed to start framework: " + std::string(e.what()));
    }
}

TXResult<void> TXAsyncProcessingFramework::stop() {
    FrameworkState expected = FrameworkState::RUNNING;
    if (!state_.compare_exchange_strong(expected, FrameworkState::STOPPING)) {
        expected = FrameworkState::PAUSED;
        if (!state_.compare_exchange_strong(expected, FrameworkState::STOPPING)) {
            return Err(TXErrorCode::Unknown, "Framework is not in RUNNING or PAUSED state");
        }
    }

    try {
        // è®¾ç½®åœæ­¢æ ‡å¿—
        should_stop_.store(true);

        // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹ç»“æŸ
        for (auto& thread : worker_threads_) {
            if (thread.joinable()) {
                thread.join();
            }
        }
        worker_threads_.clear();

        state_.store(FrameworkState::STOPPED);
        should_stop_.store(false);

        return Ok();

    } catch (const std::exception& e) {
        return Err(TXErrorCode::Unknown, "Failed to stop framework: " + std::string(e.what()));
    }
}

TXResult<void> TXAsyncProcessingFramework::pause() {
    FrameworkState expected = FrameworkState::RUNNING;
    if (!state_.compare_exchange_strong(expected, FrameworkState::PAUSED)) {
        return Err(TXErrorCode::Unknown, "Framework is not in RUNNING state");
    }

    return Ok();
}

TXResult<void> TXAsyncProcessingFramework::resume() {
    FrameworkState expected = FrameworkState::PAUSED;
    if (!state_.compare_exchange_strong(expected, FrameworkState::RUNNING)) {
        return Err(TXErrorCode::Unknown, "Framework is not in PAUSED state");
    }

    return Ok();
}

TXResult<void> TXAsyncProcessingFramework::waitForAll(std::chrono::milliseconds timeout) {
    auto start_time = std::chrono::steady_clock::now();

    while (true) {
        // æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
        if (normal_queue_->empty() && high_priority_queue_->empty()) {
            // ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
            std::this_thread::sleep_for(std::chrono::milliseconds(10));
            if (normal_queue_->empty() && high_priority_queue_->empty()) {
                break;
            }
        }

        // æ£€æŸ¥è¶…æ—¶
        auto current_time = std::chrono::steady_clock::now();
        if (std::chrono::duration_cast<std::chrono::milliseconds>(current_time - start_time) >= timeout) {
            return Err(TXErrorCode::Unknown, "Timeout waiting for all tasks to complete");
        }

        std::this_thread::sleep_for(std::chrono::milliseconds(1));
    }

    return Ok();
}

TXResult<void> TXAsyncProcessingFramework::waitForCount(size_t count, std::chrono::milliseconds timeout) {
    auto start_time = std::chrono::steady_clock::now();
    size_t initial_completed = 0;

    {
        std::lock_guard<std::mutex> lock(stats_mutex_);
        initial_completed = stats_.total_tasks_completed;
    }

    while (true) {
        size_t current_completed = 0;
        {
            std::lock_guard<std::mutex> lock(stats_mutex_);
            current_completed = stats_.total_tasks_completed;
        }

        if (current_completed - initial_completed >= count) {
            break;
        }

        // æ£€æŸ¥è¶…æ—¶
        auto current_time = std::chrono::steady_clock::now();
        if (std::chrono::duration_cast<std::chrono::milliseconds>(current_time - start_time) >= timeout) {
            return Err(TXErrorCode::Unknown, "Timeout waiting for " + std::to_string(count) + " tasks to complete");
        }

        std::this_thread::sleep_for(std::chrono::milliseconds(1));
    }

    return Ok();
}

size_t TXAsyncProcessingFramework::clearQueue() {
    size_t cleared = 0;
    
    // æ¸…ç©ºæ™®é€šä¼˜å…ˆçº§é˜Ÿåˆ—
    std::unique_ptr<TXAsyncTask> task;
    while (normal_queue_->dequeue(task)) {
        cleared++;
    }
    
    // æ¸…ç©ºé«˜ä¼˜å…ˆçº§é˜Ÿåˆ—
    while (high_priority_queue_->dequeue(task)) {
        cleared++;
    }
    
    return cleared;
}

TXAsyncProcessingFramework::FrameworkStats TXAsyncProcessingFramework::getStats() const {
    std::lock_guard<std::mutex> lock(stats_mutex_);
    
    FrameworkStats current_stats = stats_;
    
    // è®¡ç®—æ´¾ç”Ÿç»Ÿè®¡
    if (current_stats.total_tasks_completed > 0) {
        current_stats.avg_execution_time = std::chrono::microseconds(
            current_stats.total_execution_time.count() / current_stats.total_tasks_completed);
        
        auto total_seconds = current_stats.total_execution_time.count() / 1000000.0;
        if (total_seconds > 0) {
            current_stats.tasks_per_second = current_stats.total_tasks_completed / total_seconds;
        }
    }
    
    // è®¡ç®—é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ•°
    current_stats.tasks_in_queue = normal_queue_->size() + high_priority_queue_->size();
    
    // è®¡ç®—æ´»è·ƒå’Œç©ºé—²çº¿ç¨‹æ•°
    current_stats.active_worker_threads = worker_threads_.size();
    current_stats.idle_worker_threads = 0; // ç®€åŒ–å®ç°
    
    // è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
    auto memory_stats = memory_manager_.getUnifiedStats();
    current_stats.memory_usage = memory_stats.total_memory_usage;
    if (memory_stats.total_memory_usage > current_stats.peak_memory_usage) {
        current_stats.peak_memory_usage = memory_stats.total_memory_usage;
    }
    
    return current_stats;
}

void TXAsyncProcessingFramework::resetStats() {
    std::lock_guard<std::mutex> lock(stats_mutex_);
    stats_ = FrameworkStats{};
}

std::string TXAsyncProcessingFramework::generatePerformanceReport() const {
    auto stats = getStats();
    
    std::ostringstream report;
    report << "=== TXAsyncProcessingFramework æ€§èƒ½æŠ¥å‘Š ===\n";
    
    report << "\nğŸ“Š ä»»åŠ¡ç»Ÿè®¡:\n";
    report << "  æ€»æäº¤ä»»åŠ¡: " << stats.total_tasks_submitted << "\n";
    report << "  æ€»å®Œæˆä»»åŠ¡: " << stats.total_tasks_completed << "\n";
    report << "  æ€»å¤±è´¥ä»»åŠ¡: " << stats.total_tasks_failed << "\n";
    report << "  é˜Ÿåˆ—ä¸­ä»»åŠ¡: " << stats.tasks_in_queue << "\n";
    
    report << "\nâš¡ æ€§èƒ½æŒ‡æ ‡:\n";
    report << "  å¹³å‡æ‰§è¡Œæ—¶é—´: " << stats.avg_execution_time.count() << " Î¼s\n";
    report << "  ä»»åŠ¡å¤„ç†é€Ÿç‡: " << std::fixed << std::setprecision(2) << stats.tasks_per_second << " ä»»åŠ¡/ç§’\n";
    
    report << "\nğŸ§µ çº¿ç¨‹ç»Ÿè®¡:\n";
    report << "  æ´»è·ƒå·¥ä½œçº¿ç¨‹: " << stats.active_worker_threads << "\n";
    report << "  ç©ºé—²å·¥ä½œçº¿ç¨‹: " << stats.idle_worker_threads << "\n";
    
    report << "\nğŸ’¾ å†…å­˜ç»Ÿè®¡:\n";
    report << "  å½“å‰å†…å­˜ä½¿ç”¨: " << (stats.memory_usage / 1024.0 / 1024.0) << " MB\n";
    report << "  å³°å€¼å†…å­˜ä½¿ç”¨: " << (stats.peak_memory_usage / 1024.0 / 1024.0) << " MB\n";
    
    report << "\nğŸ”„ å·¥ä½œçªƒå–ç»Ÿè®¡:\n";
    report << "  å·¥ä½œçªƒå–äº‹ä»¶: " << stats.work_stealing_events << "\n";
    report << "  ä¼˜å…ˆçº§æå‡: " << stats.priority_promotions << "\n";
    
    return report.str();
}

double TXAsyncProcessingFramework::getCurrentTaskRate() const {
    auto stats = getStats();
    return stats.tasks_per_second;
}

TXResult<void> TXAsyncProcessingFramework::updateConfig(const FrameworkConfig& config) {
    if (state_.load() == FrameworkState::RUNNING) {
        return Err(TXErrorCode::Unknown, "Cannot update config while framework is running");
    }

    config_ = config;

    // é‡æ–°åˆ›å»ºé˜Ÿåˆ—å¦‚æœå®¹é‡æ”¹å˜
    if (normal_queue_->size() != config_.task_queue_capacity) {
        normal_queue_ = std::make_unique<TXLockFreeQueue<std::unique_ptr<TXAsyncTask>>>(config_.task_queue_capacity);
    }

    if (high_priority_queue_->size() != config_.high_priority_queue_capacity) {
        high_priority_queue_ = std::make_unique<TXLockFreeQueue<std::unique_ptr<TXAsyncTask>>>(config_.high_priority_queue_capacity);
    }

    return Ok();
}

TXResult<void> TXAsyncProcessingFramework::adjustWorkerThreads(size_t new_count) {
    if (state_.load() == FrameworkState::RUNNING) {
        return Err(TXErrorCode::Unknown, "Cannot adjust threads while framework is running");
    }

    config_.worker_thread_count = new_count;
    return Ok();
}

bool TXAsyncProcessingFramework::enqueueTask(std::unique_ptr<TXAsyncTask> task) {
    if (state_.load() != FrameworkState::RUNNING) {
        return false;
    }
    
    // æ£€æŸ¥å†…å­˜é™åˆ¶
    if (config_.enable_memory_management) {
        auto memory_stats = memory_manager_.getUnifiedStats();
        size_t memory_limit = config_.memory_limit_mb * 1024 * 1024;
        if (memory_stats.total_memory_usage > memory_limit * config_.memory_pressure_threshold) {
            // å†…å­˜å‹åŠ›è¿‡å¤§ï¼Œæ‹’ç»æ–°ä»»åŠ¡
            return false;
        }
    }
    
    bool enqueued = false;
    
    // æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©é˜Ÿåˆ—
    if (config_.enable_priority_scheduling && task->getPriority() > 0) {
        enqueued = high_priority_queue_->enqueue(std::move(task));
    } else {
        enqueued = normal_queue_->enqueue(std::move(task));
    }
    
    if (enqueued) {
        std::lock_guard<std::mutex> lock(stats_mutex_);
        stats_.total_tasks_submitted++;
    }
    
    return enqueued;
}

void TXAsyncProcessingFramework::workerThreadFunction(size_t thread_id) {
    while (!should_stop_.load()) {
        // æ£€æŸ¥æ˜¯å¦æš‚åœ
        if (state_.load() == FrameworkState::PAUSED) {
            std::this_thread::sleep_for(config_.worker_idle_timeout);
            continue;
        }
        
        // å°è¯•è·å–ä»»åŠ¡
        std::unique_ptr<TXAsyncTask> task = dequeueTask();
        
        if (!task) {
            // æ²¡æœ‰ä»»åŠ¡ï¼ŒçŸ­æš‚ä¼‘çœ 
            std::this_thread::sleep_for(config_.worker_idle_timeout);
            continue;
        }
        
        // æ‰§è¡Œä»»åŠ¡
        auto start_time = std::chrono::steady_clock::now();
        
        try {
            task->execute();
            
            auto end_time = std::chrono::steady_clock::now();
            auto execution_time = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);
            
            // æ›´æ–°ç»Ÿè®¡
            updateStats(*task, execution_time);
            
        } catch (const std::exception&) {
            // ä»»åŠ¡æ‰§è¡Œå¤±è´¥
            std::lock_guard<std::mutex> lock(stats_mutex_);
            stats_.total_tasks_failed++;
        }
    }
}

std::unique_ptr<TXAsyncTask> TXAsyncProcessingFramework::dequeueTask() {
    std::unique_ptr<TXAsyncTask> task;
    
    // ä¼˜å…ˆä»é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—è·å–ä»»åŠ¡
    if (config_.enable_priority_scheduling && high_priority_queue_->dequeue(task)) {
        return task;
    }
    
    // ä»æ™®é€šé˜Ÿåˆ—è·å–ä»»åŠ¡
    if (normal_queue_->dequeue(task)) {
        return task;
    }
    
    // å·¥ä½œçªƒå–ï¼ˆç®€åŒ–å®ç°ï¼‰
    if (config_.enable_work_stealing) {
        // è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å·¥ä½œçªƒå–é€»è¾‘
        std::lock_guard<std::mutex> lock(stats_mutex_);
        stats_.work_stealing_events++;
    }
    
    return nullptr;
}

void TXAsyncProcessingFramework::updateStats(const TXAsyncTask& task, std::chrono::microseconds execution_time) {
    std::lock_guard<std::mutex> lock(stats_mutex_);
    
    stats_.total_tasks_completed++;
    stats_.total_execution_time += execution_time;
    
    // æ›´æ–°å†…å­˜ç»Ÿè®¡
    if (config_.enable_memory_management) {
        auto memory_stats = memory_manager_.getUnifiedStats();
        stats_.memory_usage = memory_stats.total_memory_usage;
        if (memory_stats.total_memory_usage > stats_.peak_memory_usage) {
            stats_.peak_memory_usage = memory_stats.total_memory_usage;
        }
    }
    
    // é€šçŸ¥ç­‰å¾…çš„çº¿ç¨‹
    completion_cv_.notify_all();
}

size_t TXAsyncProcessingFramework::getOptimalThreadCount() const {
    // è·å–ç¡¬ä»¶å¹¶å‘æ•°
    size_t hardware_threads = std::thread::hardware_concurrency();
    
    if (hardware_threads == 0) {
        hardware_threads = 4; // é»˜è®¤å€¼
    }
    
    // æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´
    // å¯¹äºI/Oå¯†é›†å‹ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤šçº¿ç¨‹
    // å¯¹äºCPUå¯†é›†å‹ä»»åŠ¡ï¼Œä½¿ç”¨ç¡¬ä»¶çº¿ç¨‹æ•°
    return std::min(hardware_threads * 2, static_cast<size_t>(16)); // æœ€å¤š16ä¸ªçº¿ç¨‹
}

} // namespace TinaXlsx
